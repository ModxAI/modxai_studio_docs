# 训练打包模块详解

> v1.0.0
> 2025-12

> 📌 **模块定位**：将训练产物转换为可部署的 GGUF 格式，支持权重合并、格式转换和量化压缩。

## 目录
- [概述](#概述)
- [配置模板（推荐）](#配置模板推荐)
  - [模板一览](#模板一览)
  - [模板详解](#模板详解)
- [基础配置参数](#基础配置参数)
- [打包选项详解](#打包选项详解)
- [选项依赖关系](#选项依赖关系)
- [打包流程](#打包流程)
- [打包日志](#打包日志)
- [常见问题](#常见问题)
- [使用建议](#使用建议)
- [后续文档](#后续文档)

---

## 概述

打包模块负责将训练完成的模型（包括基础模型和 LoRA 权重）转换为适合推理部署的 GGUF 格式。由于参数较多且存在互斥和依赖关系，**强烈建议使用预设模板**进行操作。

### 核心功能

| 功能 | 说明 |
|------|------|
| LoRA 权重合并 | 将 LoRA 适配器权重合并到基础模型 |
| GGUF 格式转换 | 将 HuggingFace 格式模型转换为 GGUF |
| 模型量化 | 压缩模型体积，支持多种量化精度 |
| LoRA 独立导出 | 单独将 LoRA 权重导出为 GGUF 格式 |

### 输出产物

| 文件类型 | 命名格式 | 说明 |
|----------|----------|------|
| 未量化 GGUF | `{model_name}.gguf` | 原始精度的 GGUF 文件 |
| 量化 GGUF | `{model_name}-{Q_TYPE}.gguf` | 量化后的 GGUF 文件 |
| LoRA GGUF | `{checkpoint_name}-lora.gguf` | LoRA 适配器的 GGUF 文件 |

---

## 配置模板（推荐）

> 💡 **建议**：由于打包选项之间存在复杂的依赖关系，强烈建议使用预设模板。

### 模板一览

| 模板 | 说明 | 适用场景 |
|------|------|----------|
| **合并并量化（默认）** | 合并 LoRA + 导出 GGUF + 量化 Q4_K_M | SFT 训练后的标准打包流程 |
| **合并不量化** | 合并 LoRA + 导出 GGUF（无量化） | 保留完整精度，后续自行量化 |
| **仅量化基础模型** | 导出 GGUF + 量化（不合并 LoRA） | 预训练模型或全量微调 |
| **仅导出 LoRA** | 仅导出 LoRA 适配器为 GGUF | 需要独立使用 LoRA 的场景 |
| **自定义** | 完全自主控制所有选项 | 高级用户，了解参数关系 |

---

### 模板详解

#### 模板1：合并基础模型和 LoRA 权重，并量化（默认）

**选项配置**：
| 选项 | 值 |
|------|-----|
| 合并 LoRA 权重到基础模型 | ✅ 开启 |
| 导出基础模型为 GGUF 格式 | ✅ 开启 |
| 量化模型 | ✅ 开启 |
| 量化类型 | Q4_K_M |
| 导出 LoRA 为 GGUF 格式 | ❌ 关闭 |

**执行流程**：
```
基础模型 + LoRA 权重 → 合并 → 转换 GGUF → 量化 → 输出
```

**输出文件**：
- `merged_model.gguf`（未量化版本）
- `merged_model-Q4_K_M.gguf`（量化版本）

**适用场景**：
- SFT 微调训练后的标准打包
- 需要部署到本地推理引擎

---

#### 模板2：合并基础模型和 LoRA 权重，不量化

**选项配置**：
| 选项 | 值 |
|------|-----|
| 合并 LoRA 权重到基础模型 | ✅ 开启 |
| 导出基础模型为 GGUF 格式 | ✅ 开启 |
| 量化模型 | ❌ 关闭 |
| 导出 LoRA 为 GGUF 格式 | ❌ 关闭 |

**执行流程**：
```
基础模型 + LoRA 权重 → 合并 → 转换 GGUF → 输出
```

**输出文件**：
- `merged_model.gguf`（完整精度）

**适用场景**：
- 需要保留完整精度
- 计划使用其他工具进行量化
- 有充足的显存/内存运行完整模型

---

#### 模板3：仅量化基础模型

**选项配置**：
| 选项 | 值 |
|------|-----|
| 合并 LoRA 权重到基础模型 | ❌ 关闭 |
| 导出基础模型为 GGUF 格式 | ✅ 开启 |
| 量化模型 | ✅ 开启 |
| 量化类型 | Q4_K_M |
| 导出 LoRA 为 GGUF 格式 | ❌ 关闭 |

**执行流程**：
```
基础模型 → 转换 GGUF → 量化 → 输出
```

**输出文件**：
- `{model_name}.gguf`（未量化版本）
- `{model_name}-Q4_K_M.gguf`（量化版本）

**适用场景**：
- 预训练（Pretrain）任务的模型打包
- 全量微调（非 LoRA）的模型打包
- 仅需要量化已有的 HuggingFace 模型

---

#### 模板4：仅导出 LoRA

**选项配置**：
| 选项 | 值 |
|------|-----|
| 合并 LoRA 权重到基础模型 | ❌ 关闭 |
| 导出基础模型为 GGUF 格式 | ❌ 关闭 |
| 量化模型 | ❌ 关闭 |
| 导出 LoRA 为 GGUF 格式 | ✅ 开启 |

**执行流程**：
```
LoRA 权重 → 转换 LoRA GGUF → 输出
```

**输出文件**：
- `{checkpoint_name}-lora.gguf`

**适用场景**：
- 需要独立分发 LoRA 权重
- 基础模型已有 GGUF 版本，仅需补充 LoRA
- 测试不同 LoRA 组合

---

## 基础配置参数

### 训练任务选择 / Training task selection

| 属性 | 值 |
|------|-----|
| 类型 | 下拉选择 |
| 必填 | ✅ 是 |

**说明**：选择要打包的训练任务。

**可选任务**：
- 状态为"已完成"的任务
- 状态为"已停止"的任务
- 状态为"失败"的任务（部分场景可用）

**选择后自动填充**：
- 基础模型路径
- 输出目录
- LoRA 检查点列表（如果是 LoRA 任务）

---

### 基础模型路径 / Base model path

| 属性 | 值 |
|------|-----|
| 类型 | 目录路径 |
| 必填 | ✅ 是 |

**说明**：HuggingFace 格式的基础模型目录。

**自动填充**：选择任务后会自动填充为任务的训练模型路径。

**要求**：
- 目录中需包含 `.safetensors` 格式的模型权重
- 需包含 `config.json` 等配置文件
- 建议包含 `tokenizer.model`（SentencePiece 分词器文件）

> ⚠️ **注意**：如果目录中缺少 `tokenizer.model`，系统会尝试从基础模型路径回溯查找，但可能失败。

---

### LoRA 检查点选择 / LoRA checkpoint selection

| 属性 | 值 |
|------|-----|
| 类型 | 下拉选择 |
| 必填 | 仅 LoRA 任务 |
| 条件显示 | 仅当任务类型为 SFT 且使用 LoRA 时显示 |

**说明**：选择要打包的 LoRA 检查点。

**检查点来源**：任务输出目录下的 `checkpoint-*` 子目录。

**选择建议**：
- 参考评估模块的分数选择最佳检查点
- 通常选择训练末期的检查点（步数较大）
- 如有早停，选择早停前的最后一个检查点

---

### 输出目录 / Output directory

| 属性 | 值 |
|------|-----|
| 类型 | 目录路径 |
| 必填 | ✅ 是 |

**说明**：打包产物的输出目录。

**自动填充**：默认为任务输出目录。

**建议**：
- 使用专门的目录存放打包结果
- 确保磁盘空间充足（量化后文件仍较大）

---

## 打包选项详解

### 合并 LoRA 权重到基础模型 / Merge LoRA weights to base model

| 属性 | 值 |
|------|-----|
| 类型 | 开关 |
| 默认值 | 开启 |
| 自定义模式 | ✅ 可修改 |
| 模板模式 | ❌ 锁定 |

**说明**：将 LoRA 适配器权重合并到基础模型中。

**开启后**：
- 生成完整的模型权重
- 推理时无需单独加载 LoRA
- 文件体积等于完整模型

**关闭时**：
- LoRA 权重保持独立
- 需要搭配基础模型使用
- 支持动态切换不同 LoRA

---

### 导出基础模型为 GGUF 格式 / Export base model as GGUF

| 属性 | 值 |
|------|-----|
| 类型 | 开关 |
| 默认值 | 开启 |
| 自定义模式 | ✅ 可修改 |
| 模板模式 | ❌ 锁定 |

**说明**：将 HuggingFace 格式模型转换为 GGUF 格式。

**GGUF 格式优势**：
- llama.cpp 原生支持
- 单文件包含所有权重和配置
- 支持增量加载，降低内存需求

**转换过程**：
1. 读取 HuggingFace 模型权重
2. 转换张量格式
3. 打包分词器配置
4. 写入 GGUF 单文件

---

### 量化模型 / Quantize model

| 属性 | 值 |
|------|-----|
| 类型 | 开关 |
| 默认值 | 开启 |
| 自定义模式 | ✅ 可修改 |
| 模板模式 | ❌ 锁定 |
| 依赖条件 | 需要先开启"导出 GGUF" |

**说明**：对 GGUF 模型进行量化压缩。

**量化效果**：
- 大幅减小模型体积
- 降低推理时的内存需求
- 适度损失精度（通常可接受）

---

### 量化类型 / Quantization types

| 属性 | 值 |
|------|-----|
| 类型 | 多选下拉 |
| 默认值 | Q4_K_M |
| 条件显示 | 仅当"量化模型"开启时显示 |

**说明**：选择一种或多种量化精度。

#### 量化类型详解

**K-Quant 系列（推荐）**：

| 类型 | 大小 | 精度损失 | 推荐场景 |
|------|------|----------|----------|
| **Q4_K_M** ⭐ | ~4.6G | 低 | **默认推荐**，平衡体积和质量 |
| **Q5_K_M** ⭐ | ~5.3G | 极低 | 追求更高质量 |
| Q4_K_S | ~4.4G | 低 | 体积优先 |
| Q5_K_S | ~5.2G | 极低 | 质量优先 |
| Q3_K_M | ~3.7G | 中等 | 极限压缩 |
| Q3_K_L | ~4.0G | 中低 | 压缩与质量折中 |
| Q3_K_S | ~3.4G | 中等 | 最小 3-bit |

**标准量化系列**：

| 类型 | 大小 | 说明 |
|------|------|------|
| Q8_0 | ~8.0G | 8-bit 量化，质量接近原始 |
| Q5_0 | ~5.2G | 5-bit 标准量化 |
| Q5_1 | ~5.7G | 5-bit 增强量化 |
| Q4_0 | ~4.3G | 4-bit 标准量化 |
| Q4_1 | ~4.8G | 4-bit 增强量化 |
| Q2_K | ~3.0G | 2-bit 极限压缩 |

**IQ 系列（智能量化）**：

| 类型 | bpw | 说明 |
|------|-----|------|
| IQ4_XS | 4.25 | 智能 4-bit |
| IQ4_NL | 4.50 | 非线性 4-bit |
| IQ3_M | 3.66 | 智能 3-bit 中等 |
| IQ3_S | 3.44 | 智能 3-bit 小型 |
| IQ2_M | 2.70 | 智能 2-bit 中等 |
| IQ2_S | 2.50 | 智能 2-bit 小型 |
| IQ1_M | 1.75 | 智能 1-bit 中等 |
| IQ1_S | 1.56 | 智能 1-bit 小型 |

**全精度格式**：

| 类型 | 大小 | 说明 |
|------|------|------|
| F32 | ~26G | 32-bit 浮点（原始精度） |
| F16 | ~14G | 16-bit 浮点 |
| BF16 | ~14G | BFloat16 格式 |

**选择建议**：

| 场景 | 推荐类型 |
|------|----------|
| 通用部署 | Q4_K_M（默认） |
| 追求质量 | Q5_K_M 或 Q8_0 |
| 内存受限 | Q3_K_M 或 IQ3_M |
| 极限压缩 | Q2_K 或 IQ2_S |
| 多版本分发 | 选择多个类型同时生成 |

---

### 导出 LoRA 为 GGUF 格式 / Export LoRA as GGUF

| 属性 | 值 |
|------|-----|
| 类型 | 开关 |
| 默认值 | 关闭 |
| 自定义模式 | ✅ 可修改 |
| 模板模式 | ❌ 锁定 |

**说明**：将 LoRA 适配器单独导出为 GGUF 格式。

**使用场景**：
- 基础模型已有 GGUF 版本
- 需要分发轻量级的 LoRA 文件
- 测试不同 LoRA 的效果

**输出文件**：
- `{checkpoint_name}-lora.gguf`（体积很小，通常 MB 级别）

---

## 选项依赖关系

### 互斥关系

| 选项 A | 选项 B | 关系说明 |
|--------|--------|----------|
| 合并 LoRA | 仅导出 LoRA | 二选一，合并后无独立 LoRA |

### 依赖关系

| 前置条件 | 依赖选项 | 说明 |
|----------|----------|------|
| 导出 GGUF = 开启 | 量化模型 | 量化需要先有 GGUF 文件 |
| 量化模型 = 开启 | 量化类型选择 | 开启量化后才显示类型选择 |
| LoRA 任务 | LoRA 检查点选择 | 非 LoRA 任务不显示 |

### 流程依赖

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│  合并 LoRA  │ ──▶ │  导出 GGUF  │ ──▶ │    量化    │
└─────────────┘     └─────────────┘     └─────────────┘
      │                                        │
      │                                        ▼
      │                               ┌─────────────────┐
      │                               │ 量化类型(可多选) │
      │                               └─────────────────┘
      ▼
┌─────────────────┐
│  导出 LoRA GGUF │ （与合并互斥）
└─────────────────┘
```

---

## 打包流程

### 标准流程（使用模板）

1. **选择任务**
   - 从下拉列表选择已完成的训练任务
   - 系统自动填充路径信息

2. **选择模板**
   - 根据需求选择合适的预设模板
   - 确认右侧显示的配置摘要

3. **选择检查点**（LoRA 任务）
   - 从检查点列表选择要打包的版本
   - 参考评估分数选择最佳检查点

4. **确认输出目录**
   - 检查输出目录是否正确
   - 确保有足够的磁盘空间

5. **开始打包**
   - 点击"开始打包"按钮
   - 等待打包完成

### 自定义流程

1. **选择任务和检查点**

2. **选择"自定义"模板**
   - 此时所有选项变为可编辑状态

3. **配置各项选项**
   - 根据需求逐项设置
   - 注意选项之间的依赖关系

4. **开始打包**

---

## 打包日志

打包过程中会显示实时日志：

| 日志类型 | 说明 |
|----------|------|
| Merge LoRA | LoRA 权重合并过程 |
| GGUF conversion | 格式转换过程 |
| Quantization | 量化过程 |
| Command success | 步骤完成 |

---

## 常见问题

### 打包失败：找不到 tokenizer.model

**原因**：LoRA 合并后的目录缺少分词器文件。

**解决方法**：
1. 确保基础模型目录包含 `tokenizer.model`
2. 手动复制 `tokenizer.model` 到合并后的目录
3. 重新执行打包

### 量化失败

**可能原因**：
- GGUF 文件未正确生成
- 磁盘空间不足
- 量化工具缺失

**解决方法**：
1. 检查前一步 GGUF 转换是否成功
2. 确保有足够的磁盘空间（至少模型大小的 2 倍）
3. 检查 `llama-quantize` 工具是否存在

### 输出文件体积过大

**解决方法**：
- 选择更激进的量化类型（如 Q3_K_M、Q2_K）
- 删除不需要的中间文件（未量化的 .gguf）

### LoRA 检查点列表为空

**可能原因**：
- 任务输出目录不存在
- 训练未产生检查点
- 非 LoRA 任务

**解决方法**：
1. 确认任务确实使用了 LoRA 训练
2. 检查任务输出目录下是否有 `checkpoint-*` 目录
3. 确保训练时设置了保存检查点

---

## 使用建议

### 快速开始

对于大多数用户：
1. 选择训练任务
2. 使用默认模板（合并并量化）
3. 选择检查点
4. 开始打包

### 进阶使用

- **多量化版本**：选择多个量化类型，一次生成多个版本
- **保留原始精度**：使用"合并不量化"模板，后续用其他工具量化
- **独立 LoRA**：使用"仅导出 LoRA"模板，配合已有基础模型使用

### 部署建议

| 部署平台 | 推荐量化类型 |
|----------|--------------|
| 高端 GPU (24G+) | Q8_0 或 F16 |
| 中端 GPU (12G) | Q5_K_M |
| 低端 GPU (8G) | Q4_K_M |
| CPU 推理 | Q4_K_M 或 Q3_K_M |
| 移动端 | IQ3_M 或 IQ2_S |

---

## 后续文档

- [概述](./overview.md) - 训练功能总览
- [准备模块详解](./prepare.md) - 任务创建和参数配置
- [监控模块详解](./monitor.md) - 训练监控
- [评估模块详解](./evaluate.md) - 检查点评估
- [测试模块详解](./test.md) - 交互式测试
