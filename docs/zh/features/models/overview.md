# 模型库

> v1.0.0
> 2025-12

本文介绍 ModxAI Studio 中**模型库**功能的使用方法,帮助您完成模型导入、下载、加载、管理和自定义导出等任务。

## 目录

- [环境要求](#环境要求)
- [界面布局](#界面布局)
- [模型分类](#模型分类)
- [模型导入](#模型导入)
- [模型下载](#模型下载)
- [模型加载与卸载](#模型加载与卸载)
- [模型管理](#模型管理)
- [自定义模型导出](#自定义模型导出)
- [系统资源监控](#系统资源监控)
- [注意事项](#注意事项)

---

## 环境要求

### 外部环境依赖

以下模型类型在**加载前**需要在**设置**中安装环境：

- **向量模型（Embedding）**：用于 RAG 检索，需要 CPU 或 GPU 环境
- **重排模型（Rerank）**：用于检索结果优化，需要 CPU 或 GPU 环境
- **SD 模型（图像生成）**：需要 CPU 或 GPU 环境
- **自定义模型导出**：需要 CPU 或 GPU 环境

### 免环境依赖

以下模型类型**无需**安装依赖环境，可直接使用：

- **聊天模型（Chat）**：使用 llama.cpp 推理引擎
- **多模态模型（Multimodal）**：使用 llama.cpp 推理引擎

### 环境安装

如需使用依赖 Python 的功能，请前往**设置**页面安装 CPU 或 GPU 环境。


> **GPU 加速库（重要）**
>
> - NVIDIA（推荐）：
>   - 若使用 NVIDIA 显卡，建议通过“模型库 → 顶部右侧 ⚡ 按钮 → 安装 GPU 加速库”下载并安装对应的 CUDA 加速库。聊天/多模态模型在 NVIDIA+CUDA 下性能最佳；若要使用 Stable Diffusion（SD，图像生成）功能，SD 当前仅支持 CUDA 环境，必须安装 CUDA 加速库后才能运行 SD 推理。
>   - 安装或更新加速库后，无需重启应用.下次加载模型时会自动生效。
> - AMD（限制）：
>   - AMD 显卡可使用内置的 Vulkan 加速以运行聊天/多模态模型（无需额外加速库），但 SD 推理当前不支持 AMD/Vulkan，因此无法在 AMD 显卡上使用 SD 功能。
> - 无独立显卡或内置显卡（Intel）：
>   - 若系统未检测到独立 NVIDIA 显卡，则“安装 GPU 加速库”按钮不会显示；可继续使用 CPU 或内置 Vulkan（在受支持的场景下）运行聊天模型，但性能会受限。

---

## 界面布局

模型库页面分为以下几个区域：

### 顶部工具栏

- **导入按钮**：手动导入本地模型文件
- **下载按钮**：打开模型下载器，从 Hugging Face 或 ModelScope 下载模型
- **加载按钮**：加载选中的模型到内存
- **卸载按钮**：卸载当前已加载的模型
- **刷新按钮**：刷新模型列表和状态
- **创建模型按钮**：打开自定义模型导出界面

### 模型分类标签页

- **聊天模型**：文本对话模型
- **向量模型**：用于 RAG 检索或生成 RAG 数据集需要的嵌入模型
- **重排模型**：用于检索结果重新排序
- **多模态模型**：支持图文理解
- **SD 模型**：Stable Diffusion 图像生成模型

### 模型列表表格

显示当前分类下的所有模型，包括：

- **名称**：模型名称（可点击编辑）
- **描述**：模型描述（可点击编辑）
- **类型**：模型格式（如 GGUF、SafeTensors）
- **状态**：加载状态（已加载/未加载）
- **操作**：上下文设置 组件配置 删除等

### 系统资源监控

页面底部显示实时资源占用：

- **CPU**：CPU 使用率
- **内存**：内存使用情况
- **GPU**：GPU 显存和温度（如有 GPU）

---

## 模型分类

### 聊天模型（Chat）

**用途**：文本对话、代码生成、生成SFT数据集时AI相关节点所需的模型

**格式支持**：
- `.gguf`：llama.cpp 量化格式（推荐）
- `.bin`：GGML 格式

**环境要求**：无需在**设置**中安装环境

**配额消耗**：免费

**特性**：
- 支持 LoRA 适配器
- 支持自定义上下文大小（n_ctx）
- 支持请求队列处理（最多排队 50 个请求）
- SFT 数据生成可复用已加载的聊天模型，无需重复加载

**模型复用机制**：

当您在进行 SFT 数据生成时，如果已有聊天模型在运行：
- 系统会自动检测模型配置是否匹配（基础模型 + LoRA）
- 若匹配，SFT 任务会复用该模型，请求进入队列排队处理
- SFT 任务完成后不会卸载模型，您的聊天会话不受影响
- 多个请求通过队列机制串行处理，互不干扰

### 向量模型（Embedding）

**用途**：文本聊天时开启RAG后支持检索、文本相似度计算. 以及生成RAG数据集时生成向量处理节点所需的嵌入模型

**格式支持**：
- `.safetensors`：HuggingFace SafeTensors 格式

**环境要求**：需要安装 CPU 或 GPU 环境

**配额消耗**：加载成功后扣除配额

**特性**：
- 自动检测模型维度
- 支持批量编码

**使用建议**：

⚠️ **需要与重排序模型搭配使用**，才能提高知识库检索的速度和精度。不使用重排模型时，会对检索速度产生较大影响。

向量模型负责初步召回相关文档，重排模型负责精确排序，两者配合可以：
- 提升检索准确率：重排模型能更精准地评估相关性
- 加快检索速度：向量模型快速召回，重排模型精细排序
- 优化资源占用：合理分工，避免单一模型负担过重

### 重排模型（Rerank）

**用途**：优化检索结果排序

**格式支持**：
- `.safetensors`：HuggingFace SafeTensors 格式

**环境要求**：需要安装 CPU 或 GPU 环境

**配额消耗**：加载成功后扣除配额

**特性**：
- 支持跨编码器（Cross-Encoder）架构
- 提升检索准确率

**使用建议**：

⚠️ **建议安装 GPU 环境后再使用**，否则会影响处理速度。

重排模型采用交叉编码器架构，计算复杂度较高：
- CPU 环境：处理速度较慢，可能影响 RAG 检索体验
- GPU 环境：推理速度显著提升，实时检索更流畅

### 多模态模型（Multimodal）

**用途**：图文理解、视觉问答

**格式支持**：
- `.gguf`：llama.cpp 量化格式（推荐）
- `.bin`：GGML 格式

**环境要求**：无需在**设置**中安装环境

**配额消耗**：加载成功后扣除配额

**特性**：
- 支持 LoRA 适配器
- 支持自定义上下文大小（n_ctx）
- 需要配合多模态投影组件（mmproj）使用

### SD 模型（Stable Diffusion）

**用途**：文生图、图生图、视频生成

**格式支持**：
- `.gguf .safetensors .bin`：主模型文件
- 需配合 VAE、CLIP、T5XXL 等组件使用

**环境要求**：需要安装 CPU 或 GPU 环境

**配额消耗**：加载成功后扣除配额

**支持的模板类型**：
- **sd_official**：Stable Diffusion 官方架构（需要 VAE + CLIP-L + CLIP-G等）
- **flux**：FLUX 架构（需要 VAE + CLIP-L + T5XXL）
- **video**：视频生成 Wan 系列（需要 VAE）

**推理能力**：
- 文生图（Text to Image）
- 图生图（Image to Image）
- 视频生成（Video）

**特性**：
- 支持 LoRA 适配器（目录下所有 lora 文件, 可多个同时使用）
- 支持分离的 VAE、CLIP、T5XXL 组件
- 支持能力标签管理

---

## 模型导入

### 导入流程

1. **点击导入按钮**：在对应的模型分类标签页下点击"导入"
2. **选择文件**：
   - **聊天/多模态**：选择 `.gguf` 或 `.bin` 文件
   - **向量/重排**：选择 `.safetensors` 文件或包含该文件的目录
   - **SD 模型**：需单独使用 SD 导入对话框
3. **自动验证**：系统会自动检测文件格式和模型类型
4. **填写信息**：
   - **名称**：模型显示名称
   - **类别**：系统会自动推断，可手动调整
   - **描述**：模型说明（可选）
5. **确认导入**：点击"导入"按钮完成

### 智能检测规则

系统会根据以下信息自动推断模型类型：

- **文件名**：包含 `vision`、`mmproj`、`multimodal` 等关键词
- **目录名**：父目录包含模型类型相关关键词
- **格式**：
  - `.gguf/.bin` → 聊天或多模态
  - `.safetensors` + 目录 → 向量或重排
  - `.gguf/.bin/.safetensors` + 单文件 → SD 模型

### 多模态投影组件（mmproj）

多模态模型需要配合投影组件使用：

- **命名规范**：文件名包含 `mmproj`
- **导入方式**：
  - 在多模态 Tab 选择主模型，如果目录中已存在 mmporj 组件会自动导入, 或手动在组件中单独导入 mmproj 组件
  - 可通过下载器下载多模态模型时会一并下载 mmproj 组件,并自动导入

### SD 模型导入

SD 模型导入较为复杂，需分步配置：

1. **选择模板类型**：sd_official、flux 或 video
2. **选择主模型文件**：Unet/Diffusion 模型文件（.gguf/.bin/.safetensors）
3. **配置组件**：
   - **VAE**：所有模板都需要
   - **T5XXL**：所有模板都需要
   - **CLIP-L**：sd_official 和 flux 需要
   - **CLIP-G**：sd_official 模型需要
4. **设置推理能力**：选择支持的生成类型（文生图/图生图/视频）
5. **填写名称和描述**
6. **确认导入**

### LoRA 适配器导入

为基础模型添加 LoRA 适配器：

1. **选中基础模型**：在模型表格中选择要绑定的基础模型
2. **点击"导入 LoRA"**：点击主模型前的"+"展开后,点击"导入 LoRA"
3. **填写信息**：
   - **名称**：LoRA 适配器名称
   - **文件路径**：选择 LoRA 文件
   - **描述**：说明（可选）
4. **验证兼容性**：系统会自动检查 LoRA 与基础模型的兼容性
5. **确认导入**

**注意**：
- 聊天/多模态模型的 LoRA 格式：`.gguf`
- SD 模型支持目录导入（包含多个 LoRA 文件）

---

## 模型下载

### 打开下载器

点击顶部工具栏的"下载"按钮，打开模型下载器窗口。

### 下载源选择

支持两个下载源：

- **ModelScope**: 中国大陆用户访问速度较快
- **Hugging Face**: 全球模型资源丰富

### 下载流程

1. **选择分类**：聊天、向量、重排、多模态、音频转文本、文生图、视频生成
2. **搜索模型**：使用关键词搜索
3. **选择模型**：点击模型卡片查看详情
4. **选择文件**：
   - 可选择整个模型（所有文件）
   - 或选择特定的量化版本
5. **设置保存路径**：选择模型保存目录
6. **开始下载**：点击"开始下载"按钮

### 下载状态

- **进度显示**：实时显示下载百分比、速度、剩余时间
- **文件计数**：已下载文件数 / 总文件数
- **暂停/继续**：可随时取消和恢复下载
- **最小化**：可将下载器最小化到按钮，后台继续下载

### 下载后处理

下载完成后，模型会自动出现在对应分类的模型列表中，无需手动导入。

---

## 模型加载与卸载

### 加载模型

1. **选择模型**：在模型表格中勾选要加载的模型
2. **可选 LoRA**：如果该模型支持 LoRA，可展开并选择 LoRA 适配器
3. **点击加载**：点击顶部的"加载"按钮
4. **环境检查**：
   - 向量/重排/SD 模型会自动检查环境安装情况
   - 如未安装，会弹窗提示前往设置页面安装
5. **等待加载**：按钮显示"加载中..."
6. **加载完成**：模型状态变为"已加载"，按钮变为"已加载模型"

### 卸载模型

1. **点击卸载按钮**：点击顶部的"卸载"按钮,对应分类下已加载的模型
2. **确认操作**：系统会提示卸载操作,并提示所需卸载的模型名称
3. **等待卸载**：按钮显示"卸载中..."
4. **卸载完成**：模型状态恢复为"未加载"

### 加载规则

- **单模型加载**：同一分类只能加载一个基础模型
- **LoRA 组合**：
  - 聊天/多模态：最多加载 1 个 LoRA
  - SD 模型：可同时加载指定目录下的多个 LoRA
- **自动卸载**：加载新模型时会自动卸载旧模型
- **配额扣除**：向量/重排/多模态/SD 模型加载成功后会扣除免费配额

### 上下文大小设置

聊天和多模态模型支持自定义上下文大小（n_ctx）：

1. **打开设置**：在操作列点击"上下文设置"
2. **输入数值**：手动输入或选择预设值
3. **特殊值**：
   - **0**：使用模型默认值（推荐）
4. **保存设置**：点击"确认"
5. **重新加载**：修改后需重新加载模型才能生效

**注意**：
- 上下文越大，占用内存越多
- 建议根据任务需求选择合适的大小
- 过大的上下文可能导致推理速度变慢

---

## 模型管理

### 编辑模型信息

**名称和描述可直接点击编辑**：

1. **点击字段**：在表格中直接点击名称或描述列
2. **进入编辑**：字段变为输入框
3. **修改内容**：输入新内容
4. **保存**：失焦或按 Enter 自动保存

### 删除模型

1. **选择模型**：确认要删除的模型
2. **检查状态**：已加载的模型无法删除，需先卸载
3. **点击删除**：在操作列点击"删除"按钮
4. **确认操作**：弹窗确认删除
5. **文件处理**：
   - **仅删除记录**：不删除实际文件
   - **文件保留**：模型文件仍在原位置

**警告**：
- 删除操作不可恢复
- 基础模型被删除后，关联的 LoRA 适配器也会一并删除

### SD 能力管理

SD 模型支持二次编辑推理能力标签：

1. **选中 SD 模型**：在 SD 模型 Tab 选中模型
2. **打开 组件 设置**：点击操作列的"组件"
3. **修改标签**：勾选或取消支持的生成类型
4. **修改组件**：添加或更换 VAE、CLIP、T5XXL 组件
5. **保存设置**：点击"确认"

### LoRA 管理

**选择 LoRA**：
- 展开基础模型，显示绑定的 LoRA 列表
- 勾选要启用的 LoRA（单选）

**删除 LoRA**：
- 在 LoRA 行点击"删除"按钮
- 确认删除操作
- LoRA 记录删除，文件保留

---

## 自定义模型导出

### 功能说明

基于配置参数从头创建并导出自定义语言模型，用于实验或特定需求。

**环境要求**：需要安装 CPU 或 GPU 环境

### 使用流程

1. **打开创建抽屉**：点击右上角"创建模型"按钮
2. **选择预设**（可选）：
   - ModxAI-Custom-Small（轻量级）
   - ModxAI-Custom-Medium（中等规模）
   - ModxAI-Custom-Large（大型）
   - ModxAI-Completely-Free（完全自定义）
3. **配置基础参数**：
   - **输出目录**：模型保存路径（必填）
   - **词表大小**：10,000 - 100,000
   - **隐藏层大小**：128 - 8192
4. **Tokenizer 配置**：
   - **默认 Tokenizer**：使用预训练 tokenizer
   - **自定义 Tokenizer**：从语料文件训练（需选择 .txt 文件）
5. **高级参数**（折叠面板）：
   - **层数**：Transformer 层数
   - **注意力头数**：多头注意力机制
   - **FFN 中间层**：前馈网络维度
   - **KV 头数**：键值对注意力头数
6. **参数预览**：
   - 实时显示**总参数量**和**预估显存占用**
   - 验证错误提示（如配置不合理）
7. **开始导出**：点击"创建模型"按钮
8. **等待完成**：导出过程可能需要几分钟

**提醒**：
- 此模型不建议直接推理使用，仅用于实验和预训练等场景


### 预设说明

| 预设 | 参数量 | 显存占用 | 适用场景 |
|------|-------|---------|---------|
| Small | ~100M | ~2GB | 轻量级测试、学习 |
| Medium | ~500M | ~4GB | 一般对话、实验 |
| Large | ~1B+ | ~8GB+ | 复杂任务、高质量 |
| Completely-Free | 动态 | 动态  | 实验性 |

### 参数验证规则

系统会自动检查以下约束：

- 注意力头数必须能整除隐藏层大小
- KV 头数必须能整除注意力头数
- 预估显存不超过系统可用内存
- 参数量在合理范围内

### 自定义 Tokenizer

如需从特定领域语料训练 tokenizer：

1. **准备语料**：纯文本文件（.txt），每行一句
2. **设置词表大小**：8,000 - 50,000（推荐 16,000 - 32,000）
3. **选择类型**：
   - **SentencePiece**（推荐）：适用于中文、日文等
   - **BPE**：适用于英文等拉丁语系
4. **开始训练**：系统会自动训练并保存 tokenizer
5. **自动应用**：训练完成后自动用于模型导出

---

## 系统资源监控

页面底部实时显示系统资源占用情况：

### CPU 监控

- **使用率**：圆环进度条显示 CPU 占用百分比
- **状态**：超过 90% 显示红色警告

### 内存监控

- **使用率**：圆环进度条显示内存占用百分比
- **具体数值**：已用 / 总容量（GB）
- **状态**：超过 90% 显示红色警告, 并会触发内存不足提示, 以及在必要时卸载模型,释放内存确保系统稳定运行

### GPU 监控

仅在检测到 GPU 时显示：

- **显存占用**：已用 / 总容量（MB）
- **温度**：实时温度（°C）
- **厂商信息**：GPU 型号和制造商

**注意**：
- NVIDIA GPU 可实时监控显存和温度
- AMD/Intel GPU 仅显示静态信息（总显存、厂商）
- 无 GPU 时不显示此卡片

---

## 注意事项

### 文件格式限制

- **聊天/多模态**：仅支持 `.gguf` 和 `.bin`
- **向量/重排**：仅支持 `.safetensors`（可选目录导入）
- **SD 模型**：仅支持 `.gguf/.safetensors/.bin`
- 不支持的格式会在验证阶段被拒绝

### 环境依赖检查

加载以下模型时会自动检查环境：

- **向量模型**：未安装会弹窗引导
- **重排模型**：未安装会弹窗引导
- **SD 模型**：未安装会弹窗引导
- **自定义导出**：未安装会弹窗引导

### 配额消耗规则

- **聊天模型**：免费，无限次加载
- **多模态模型**：加载成功后扣除配额
- **向量模型**：加载成功后扣除配额
- **重排模型**：加载成功后扣除配额
- **SD 模型**：加载成功后扣除配额

**扣除时机**：模型状态变为"已加载"时

### 模型加载限制

- **单模型限制**：同一分类只能加载一个基础模型
- **LoRA 限制**：
  - 聊天/多模态：最多 1 个 LoRA
  - SD 模型：可多个 LoRA（数量无限制，但受显存限制）
- **并发限制**：不支持聊天和多模态同时加载. 聊天,向量,重排,SD可同时加载

### 删除保护

- **已加载模型**：无法删除，必须先卸载
- **基础模型**：删除后，关联的 LoRA 一并删除
- **文件保留**：删除操作只删除数据库记录，不删除实际文件

### 性能建议

- **上下文大小**：根据任务选择，不宜过大
- **显存占用**：加载前查看系统资源监控
- **并发使用**：避免同时加载过多模型

### 常见问题

**Q: 导入后模型不显示？**
A: 请点击刷新按钮，或检查文件格式是否正确。

**Q: 加载失败怎么办？**
A: 检查环境是否安装、显存是否充足、文件是否完整。

**Q: LoRA 不兼容？**
A: LoRA 必须与基础模型架构匹配，建议使用专门训练的 LoRA。

**Q: 下载速度慢？**
A: 尝试切换下载源

**Q: 自定义模型导出失败？**
A: 检查参数配置是否合理、输出目录是否有写权限、环境是否正确安装。

---

完成本章节学习后，您可以：
- 前往**聊天**功能，使用已加载的聊天模型进行对话
- 前往**知识库**功能，配置向量和重排模型进行 RAG 检索
- 前往**数据处理**功能，使用模型生成 SFT 或 RAG 数据集
- 前往**设置**页面，管理环境和系统配置
