# SFT 数据处理（文本/代码）

> v1.0.0
> 2025-12

本文详细介绍如何使用 ModxAI Studio 的 SFT（监督微调）数据处理功能，将原始文本或代码文件转换为符合训练格式要求的高质量数据集。

---

## 目录

- [环境要求](#环境要求)
- [功能简介](#功能简介)
- [数据类型选择](#数据类型选择)
- [工作流模板](#工作流模板)
- [处理流程总览](#处理流程总览)
- [各步骤详解](#各步骤详解)
- [AI 节点配置](#ai-节点配置)
- [增量处理与错误修复](#增量处理与错误修复)
- [输出格式](#输出格式)
- [注意事项](#注意事项)
- [常见问题](#常见问题)

---

## 环境要求

**本功能需在设置中安装额外的CPU或GPU环境依赖。**
- 进入 ModxAI Studio 的**设置**页面
- 在**环境管理**标签页，选择并安装所需的环境（CPU/GPU）
- 安装完成后，返回数据处理页面即可使用相关功能

---

## 功能简介

SFT 数据处理用于将原始文本或代码转换为可用于模型微调训练的结构化数据集。系统支持两种数据类型：**文本**和**代码**，并提供多种预设工作流模板，帮助您快速完成数据准备工作。

**适用场景**：
- 领域知识微调数据集构建
- 代码补全与分析模型训练
- 问答对数据自动生成
- 预训练语料准备

**核心特性**：
- 支持 AI 辅助数据合成，自动生成高质量问答对
- 多线程并行处理，充分利用硬件资源
- 增量处理支持，可断点续传
- 完善的错误恢复机制

---

## 数据类型选择

### 文本数据（Text）

适用于处理各类文档、文章、技术资料等文本内容。

**支持的文件格式**：
- 文档类：PDF、Word（.doc/.docx）、PowerPoint（.ppt/.pptx）、Excel（.xls/.xlsx）
- 网页类：HTML（.html/.htm）
- 文本类：TXT、Markdown、CSV、JSON、XML
- 压缩包：ZIP（自动解压处理）

### 代码数据（Code）

适用于处理源代码文件，当前支持两种代码类型：

#### C/C++ 代码（Clang）

使用 Clang 编译器前端进行语法分析，能够准确识别函数、类、变量等代码结构。

**支持的文件格式**：`.c`, `.cpp`, `.cc`, `.h`, `.hpp`

**依赖要求**：需要系统安装 libclang 库，程序会自动搜索以下位置：
- 环境变量 `LIBCLANG_PATH` 指定的路径
- `runtime/packages/clang` 目录
- Python clang 包目录

#### 着色器代码（Shader）

支持多种着色器语言，特别针对 Unreal Engine 格式进行了优化。

**支持的文件格式**：`.hlsl`, `.glsl`, `.cg`, `.ush`, `.usf`, `.h`

**解析内容**：
- 文件顶部注释
- `#include` 指令
- 宏定义（`#define`）
- 结构体定义
- 函数定义
- 条件编译块（`#if/#endif`）

---

## 工作流模板

为简化操作，系统预设了几种常用的工作流模板，您可以根据需求选择：

| 模板名称 | 包含步骤 | 适用场景 | 说明 |
|----------|----------|----------|------|
| **AI 合成** | 0→1→2→3→4→5→7→10 | 需要 AI 生成问答对 | 最常用的完整流程 |
| **AI 质量** | 0→1→2→3→4→5→7→8→9→10 | 需要 AI 过滤低质量数据 | 包含质量评估过滤 |
| **预训练** | 0→1→2→3→6→7→10 | 生成预训练语料 | 跳过 AI 处理，保留原始格式 |
| **自定义** | 按需选择 | 特殊需求 | 手动选择执行步骤 |

> **关于预训练模板**：预训练数据的处理流程与 SFT 高度相似（仅输出格式不同），将其整合在同一处理界面中，通过模板选择来区分最终输出格式。

---

## 处理流程总览

SFT 数据处理包含 11 个步骤（0-10），部分步骤为可选：

| 步骤 | 名称 | 说明 | 必选 | 可配置 |
|------|------|------|------|--------|
| 0 | 转换/解析 | 读取并解析源文件 | ✓ | 代码可配置 |
| 1 | 清洗 | 清理无效内容 | ✓ | ✓ |
| 2 | 拆分 | 切分为适当大小的块 | ✓ | ✓ |
| 3 | 分析/整理 | 质量评估与复杂度分析 | ✓ | - |
| 4 | AI 合成 | 使用 AI 生成问答对 | 可选 | ✓ |
| 5 | AI 合成解析 | 解析 AI 生成结果 | 可选 | - |
| 6 | 转预训练 | 转换为预训练格式 | 可选 | - |
| 7 | 整合数据 | 汇总所有数据并格式化 | ✓ | - |
| 8 | AI 过滤 | AI 质量评估过滤 | 可选 | ✓ |
| 9 | AI 过滤解析 | 解析过滤结果 | 可选 | - |
| 10 | 输出 | 生成最终训练数据 | ✓ | ✓ |

**步骤关系说明**：
- 步骤 4-5 与步骤 6 互斥：选择 AI 合成流程则跳过预训练转换，反之亦然
- 步骤 5/9 会在执行 4/8 后自动触发，无需手动选择
- 步骤 8-9 需要步骤 7 的输出作为输入

---

## 各步骤详解

### 步骤 0：转换/解析

**作用**：读取源文件并提取内容。文本和代码的处理方式不同。

#### 文本数据处理

自动识别文件格式并提取纯文本内容，保留原有的段落结构。

#### 代码数据处理

使用专用解析器分析代码结构：

- **Clang 解析器**：识别函数、类、结构体、变量声明等
- **Shader 解析器**：识别宏、结构体、函数、include 依赖等

**可配置参数（代码）**：

| 参数 | 说明 | 默认值 |
|------|------|--------|
| **代码版本** | 标注代码版本信息（如项目版本号） | 空 |
| **排除目录** | 跳过指定目录，用逗号分隔 | 空 |

---

### 步骤 1：清洗

**作用**：清理数据中的噪音内容，提升数据质量。

**处理内容**：
- 去除页眉页脚、导航栏等重复性内容
- 清理特殊符号和乱码字符
- 过滤过短的无意义段落

**可配置参数**：

| 参数 | 说明 | 默认值 | 建议 |
|------|------|--------|------|
| **最小段落长度** | 低于此字符数的段落将被过滤 | 3 | 根据内容类型调整，技术文档可设置更大值 |
| **样本作用范围** | 清洗规则的应用范围 | 结构 | `结构` - 只清理页眉页脚等结构区域<br>`全文` - 对全文应用清洗规则 |
| **噪声短语匹配模式** | 清洗样本的匹配模式 | plain | plain=前缀匹配；wildcard=支持*通配；regex=正则匹配 |
| **噪声样本** | 自定义需要清除的内容 | 空 | 用 `\|` 分隔多个样本，如：`版权所有\|Copyright\|联系我们`<br>建议每个样本不超过 20 个字符 |
| **链接行删除级别** | 含链接行的删除级别 | 2 | 0=不删除；1=删除本地/私有路径；2=删除所有链接 |

**高级参数（折叠）**：

| 参数 | 说明 | 默认值 | 建议 |
|------|------|--------|------|
| **清理私有IP链接** | 是否清理包含私有IP的URL行 | 开启 | 建议开启，避免泄露内网地址 |
| **保留富文本链接行** | 含链接但内容丰富的行是否保留 | 关闭 | 开启可保留有价值的引用说明 |
| **脱敏代码块URL** | 是否将代码块中URL替换为[REDACTED_URL] | 关闭 | 根据安全需求决定 |
| **富文本词数阈值** | 判断富文本的词数阈值 | 12 | 用于判断是否保留含链接行 |
| **富文本长度阈值** | 判断富文本的字符长度阈值 | 80 | 配合词数阈值综合判断 |

---

### 步骤 2：拆分

**作用**：将长文本/代码切分为适合处理的数据块（Chunk）。

**处理逻辑**：
- 按自然边界（标题、段落、函数）切分
- 保持语义/代码完整性
- 超长块会二次拆分，并记录关联 hash

**可配置参数**：

| 参数 | 说明 | 默认值 | 建议 |
|------|------|--------|------|
| **最小段落长度** | 单个块的最小字符数 | 3 | - |
| **文档最小 Token** | 整个文档的最小 Token 数 | 10 | 过短文档将被跳过 |
| **块最小 Token** | 单个块的最小 Token 数 | 15 | 避免生成过碎的块 |

---

### 步骤 3：分析/整理

**作用**：对数据块进行质量分析，按复杂度分组。

**文本数据分析维度**：
- 有效内容比例（去除停用词后）
- 词汇丰富度（TTR 指标）
- 句子复杂度（平均句长）
- 结构化程度（标题、列表、代码块数量）

**代码数据分析维度**：
- 圈复杂度（控制流复杂度）
- 认知复杂度
- 嵌套深度
- Token 密度

**复杂度分组**：
- `high`：高复杂度，适合生成深度分析内容
- `mid`：中等复杂度，适合常规问答
- `low`：低复杂度，适合简单问答或跳过

> **资源提示**：此步骤为多线程并行处理，会占用较多 CPU 和内存资源，处理大量数据时请注意系统负载。

---

### 步骤 4：AI 合成（可选）

**作用**：使用 AI 模型对数据块进行分析，生成训练用的问答对或分析内容。

通过 AI 能力自动将原始文本/代码转换为分析或问答对甚至自定义风格的结构化数据。

**任务类型**：

| 类型 | 说明 | 输出内容 |
|------|------|----------|
| **分析（Analysis）** | 对内容进行深度解读 | 结构化分析文本 |
| **问答（QA）** | 生成问答对 | 问题-答案配对 |
| **全部（All）** | 同时执行两种任务 | 分析 + 问答 |

**复杂度匹配**：

系统会根据步骤 3 的复杂度分组，为不同复杂度的数据块匹配不同的 AI 提示词：

| 复杂度 | 分析重点 | QA 生成数量 |
|--------|----------|-------------|
| high | 深度剖析核心概念和实现原理 | 5-8 对 |
| mid | 说明主要功能和用法 | 3-5 对 |
| low | 简要描述基本信息 | 1-3 对 |

详细的 AI 参数配置见 [AI 节点配置](#ai-节点配置) 章节。

---

### 步骤 5：AI 合成解析（可选）

**作用**：解析步骤 4 的 AI 输出，提取结构化数据。排除无效内容。

此步骤建议在步骤 4 完成后执行，将 AI 的输出结果进行清洗，确保后续步骤使用高质量数据。

---

### 步骤 6：转预训练（可选）

**作用**：将数据转换为预训练格式，跳过 AI 问答生成。

当您选择"预训练"模板时会使用此步骤。输出格式为纯文本语料，适合用于模型的预训练或继续预训练。

> **与 AI 合成互斥**：步骤 6 与步骤 4-5 互斥，系统会根据选择的模板自动处理。

---

### 步骤 7：整合数据

**作用**：汇总前面步骤的输出，转换为 ChatML 对话格式。

**处理内容**：
- 合并同一来源的多个数据块
- 构建 System-User-Assistant 对话结构
- 处理超长内容的分块与重叠

**输出格式示例**：
```json
{
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "问题内容..."},
    {"role": "assistant", "content": "回答内容..."}
  ]
}
```

---

### 步骤 8：AI 过滤（可选，谨慎使用）

**作用**：使用 AI 对整合后的数据进行质量评估，过滤低质量条目。

**评估标准**：
- 内容准确性和相关性
- 表达清晰度
- 完整性和一致性

**输出判定**：
- `<P>`：通过（Pass），保留该条数据
- `<F>`：失败（Fail），过滤掉该条数据

> ⚠️ **重要警告**：
> 
> AI 过滤需要使用**分析能力较强的模型**（如 4B 以上参数量的模型）。如果使用能力不足的模型，可能会：
> - 错误地将高质量数据标记为低质量
> - 保留实际有问题的数据
> - 导致最终数据集质量下降
>
> **建议**：除非您有充足的计算资源和高质量模型，否则建议跳过此步骤，通过调整步骤 4 的提示词参数来控制输出质量。

---

### 步骤 9：AI 过滤解析（自动）

**作用**：解析步骤 8 的过滤结果，移除被标记为 `<F>` 的条目。

---

### 步骤 10：输出

**作用**：生成最终的训练数据文件，支持多种格式和数据集划分。

**可配置参数**：

| 参数 | 说明 | 默认值 | 选项 |
|------|------|--------|------|
| **数据集划分** | 训练/验证/测试集比例 | 0.8, 0.1, 0.1 | 三个比例之和应为 1 |
| **并行线程数** | 数据处理的并行度 | 16 | 1-64 |
| **Tokenizer 路径** | 用于 Token 计算的分词器 | 内置 | 可指定自定义路径 |
| **保存格式** | 输出文件格式 | .jsonl | `.jsonl` / `.json` / `.npy` |
| **系统提示词** | 添加到所有对话的系统消息 | 空 | 可自定义 |

**输出文件**：
- `train.jsonl`：训练集
- `eval.jsonl`：验证集
- `test.jsonl`：测试集

---

## AI 节点配置

步骤 4（AI 合成）和步骤 8（AI 过滤）都需要配置 AI 模型参数。

### 基础参数

| 参数 | 说明 | 默认值 |
|------|------|--------|
| **AI 模型** | 选择要使用的模型 | 必选 |
| **任务类型** | Analysis / QA / All | Analysis |
| **复杂度匹配** | 处理哪些复杂度级别的数据 | 全部 |

### 高级参数（折叠区域）

| 参数 | 说明 | 默认值 | 范围 |
|------|------|--------|------|
| **温度 (temperature)** | 生成多样性 | 0.7 | 0-1 |
| **最大 Token (max_tokens)** | 最大输出长度 | 1024 | 1-8192 |
| **Top K** | 采样候选数 | 20 | 0-100 |
| **Top P** | 核采样阈值 | 0.95 | 0-1 |
| **重复惩罚 (repeat_penalty)** | 避免重复 | 1.1 | 0-2 |
| **Min P** | 最小概率阈值 | 0.0 | 0-1 |
| **启用思考** | 使用思维链推理 | 否 | - |

### 自定义系统提示词

您可以在 **系统提示词** 字段中自定义 AI 的行为指导。如果留空，系统会根据任务类型使用默认提示词：

**分析任务默认提示词**（概述）：
- 作为专业技术分析师
- 对内容进行结构化深度分析
- 输出包含核心概念、关键点、代码示例（如适用）

**QA 任务默认提示词**（概述）：
- 作为专业技术文档编写者
- 生成高质量问答对
- 问题应覆盖不同难度层次

**代码分析提示词会额外包含**：
- 函数/方法的参数和返回值分析
- 代码实现原理说明
- 潜在的优化建议

---

## 增量处理与错误修复

AI 合成步骤（步骤 4）支持增量处理和自动错误修复，这在处理大量数据时非常有用。

### 增量处理

当您在处理过程中暂停或程序意外中断后重新执行时：

1. **自动检测**：系统会检测已处理完成的数据块
2. **跳过已完成**：已成功处理的块不会重复请求
3. **继续处理**：只处理新增或未完成的数据

**实现机制**：
- 使用 WAL（预写日志）记录每个处理结果
- 通过 Hash 值追踪每个数据块的处理状态
- 定期自动备份（每 15 分钟）

### 错误修复机制

当 AI 请求失败时，系统会自动进行重试和修复：

**重试策略**：
- 最大重试次数：3 次
- 重试间隔：60 秒
- 超时时间：根据 Token 数量动态调整（120-600 秒）

**错误恢复**：
- 失败的请求会被记录到错误列表
- 下次执行时自动尝试修复之前的错误
- 支持手动触发错误修复

**状态追踪**：
```json
{
  "total_groups": 100,
  "processed_count": 85,
  "processed_groups": {
    "file_a.txt__abc123": {
      "total_chunks": 10,
      "processed_chunks": ["hash1", "hash2", ...],
      "error_chunks": ["hash9"],
      "is_done": false
    }
  }
}
```

---

## 输出格式

### JSONL 格式（推荐）

每行一个 JSON 对象，符合 ChatML 格式：

```jsonl
{"messages": [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
{"messages": [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
```

### JSON 格式

完整的 JSON 数组：

```json
[
  {"messages": [...]},
  {"messages": [...]}
]
```

### NPY 格式

NumPy 二进制格式，适合直接用于训练框架加载：
- `train.npy`：Token ID 数组
- `train.npy.shape.json`：形状元数据

---

## 注意事项

### 性能与资源

1. **多线程处理**：步骤 0 和步骤 1 使用多线程并行处理，会占用较多 CPU 和内存
2. **AI 请求开销**：步骤 4 和 8 需要大量 AI 请求，处理时间取决于数据量和模型速度
3. **磁盘空间**：中间文件会占用一定磁盘空间，建议预留足够空间

### 代码处理

1. **Clang 依赖**：处理 C/C++ 代码需要 libclang 库支持
2. **编码问题**：建议源代码使用 UTF-8 编码
3. **排除目录**：对于大型项目，建议排除 build、vendor 等目录

### AI 模型选择

1. **合成步骤**：建议使用 4B 以上参数的模型
2. **过滤步骤**：建议使用 4B 以上参数的模型

---

## 常见问题

### Q: 处理过程中断了怎么办？

A: 直接重新执行相同步骤即可。系统会自动检测已处理的内容，从断点继续处理。

### Q: AI 合成生成的内容质量不理想？

A: 可以尝试以下方法：
1. 调整温度参数（降低可增加一致性）
2. 自定义系统提示词
3. 使用更大的模型
4. 调整复杂度匹配，先处理高复杂度数据

### Q: 为什么没有单独的预训练入口？

A: 预训练数据的处理流程与 SFT 高度相似，主要区别在于：
- 不需要 AI 问答生成（步骤 4-5）
- 输出格式为纯文本而非对话格式

因此将其整合在同一界面，通过选择"预训练"模板来简化操作。

### Q: AI 过滤导致数据量大幅减少？

A: 这通常意味着：
1. 使用的模型能力不足，建议更换更强的模型
2. 原始数据质量确实存在问题
3. 可以暂时跳过 AI 过滤步骤，人工抽查数据质量

### Q: 如何处理超大规模数据集？

A: 建议分批处理：
1. 将数据分成多个子目录
2. 分别处理每个子目录
3. 最后合并输出文件
4. 利用增量处理功能，可以分多次执行

### Q: Shader 代码支持哪些引擎格式？

A: 主要支持：
- 标准 HLSL/GLSL
- Unreal Engine 着色器（.ush/.usf）
- CG 着色器
- 兼容性头文件（.h）

---

*如有其他问题，请参考项目文档或在社区提问。*
