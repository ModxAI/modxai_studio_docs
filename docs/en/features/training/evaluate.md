# Training Evaluation Module Details

> v1.0.0
> 2025-12

> ðŸ“Œ **Module Positioning**: Automatically evaluate checkpoint quality and help users find the best checkpoint through standardized sample testing.

## Table of Contents
- [Overview](#overview)
- [Evaluation Samples](#evaluation-samples)
- [Evaluation Parameters](#evaluation-parameters)
- [Evaluation Flow](#evaluation-flow)
- [Evaluation Metrics](#evaluation-metrics)
- [Usage Suggestions](#usage-suggestions)
- [Difference from Test Module](#difference-from-test-module)

## Overview

The Evaluation module uses predefined Q&A samples to batch test training checkpoints, automatically calculates similarity scores, and helps users objectively evaluate model training effects.

### Core Functions

| Function | Description |
|------|------|
| Batch Evaluation | Automatically evaluate checkpoints using sample datasets |
| Score Calculation | Calculate similarity between model output and standard answers |
| Result Comparison | Visual comparison of evaluation results for multiple checkpoints |
| Best Recommendation | Automatically recommend the checkpoint with the highest score |

---

## Evaluation Samples

### Default Sample File

The system ships with a default evaluation sample file:

```
server/resources/example/train_evaluation/model_evaluation_samples.json
```

When running the evaluation function for the first time, this file will be copied to the user data directory as the default configuration.

### Sample Format

```json
{
  "train": [
    {
      "prompt": "Question text",
      "completion": "Standard answer"
    }
  ],
  "validation": [
    {
      "prompt": "Validation question",
      "completion": "Validation answer"
    }
  ]
}
```

### Sample Group Description

| Group | Purpose |
|------|------|
| `train` | Training set samples, used to evaluate the model's learning effect on training data |
| `validation` | Validation set samples, used to evaluate the model's generalization ability |

### Custom Samples

Users can modify the sample file to suit their own evaluation needs:

1. **Add Sample**: Add new `prompt`/`completion` objects to the array of the corresponding group.
2. **Modify Sample**: Edit the question and answer of existing samples.
3. **Delete Sample**: Remove unwanted sample objects.

**Sample Writing Suggestions**:

| Type | Suggestion |
|------|------|
| Question Coverage | Cover main scenarios of training data |
| Answer Standard | Use expected standard answer format |
| Quantity Balance | Appropriate balance between train and validation quantities |
| Difficulty Gradient | Include questions of different difficulties from simple to complex |

---

## Evaluation Parameters

### Sampling Control Parameters

#### Sample ratio

| Attribute | Value |
|------|-----|
| Type | Float |
| Default | 0.3 |
| Range | 0.1 - 1.0 |

**Description**: Proportion randomly drawn from the sample set.

**Usage Scenarios**:
- When there are many samples, set a lower ratio to speed up evaluation.
- When comprehensive evaluation is needed, set to 1.0 to use all samples.

---

#### Max samples

| Attribute | Value |
|------|-----|
| Type | Integer |
| Default | 20 |
| Range | 5 - 100 |

**Description**: Maximum number of samples used in a single evaluation.

**Function**: Limit evaluation time to avoid excessively long evaluation due to too many samples.

---

#### Min samples

| Attribute | Value |
|------|-----|
| Type | Integer |
| Default | 5 |
| Range | 1 - 50 |

**Description**: Ensure the minimum number of samples used for evaluation.

**Function**: Guarantee minimum sample quantity even if the sampling ratio calculation result is small.

---

### Inference Parameters

Model inference parameters during evaluation, same as regular inference parameters:

#### Temperature

| Attribute | Value |
|------|-----|
| Type | Float |
| Default | 0.3 |
| Range | 0 - 1 |

**Description**: Control output randomness.

**Evaluation Suggestion**: Use lower temperature (0.1-0.3) to obtain more stable and reproducible evaluation results.

---

#### Max length

| Attribute | Value |
|------|-----|
| Type | Integer |
| Default | 256 |
| Range | 32 - 2048 |

**Description**: Maximum number of tokens generated by the model.

**Evaluation Suggestion**: Set according to the average length of sample answers, usually 256-512 is sufficient.

---

### Advanced Parameters

The following parameters are located in the fold panel:

| Parameter | Default | Description |
|------|--------|------|
| Top K | 20 | Keep the K highest probability words during sampling |
| Top P | 0.9 | Cumulative probability threshold for nucleus sampling |
| Repetition Penalty | 1.1 | Penalty coefficient to suppress repetitive content |
| Presence Penalty | 1.1 | Penalty coefficient to suppress already appeared tokens |
| Min P | 0.0 | Minimum probability threshold |
| Random Seed | 42 | Fixed seed can reproduce evaluation results |

---

## Evaluation Flow

### 1. Select Checkpoint

Select the checkpoint to evaluate on the evaluation page:
- Can select a single checkpoint for evaluation.
- Can select multiple checkpoints for comparative evaluation.

### 2. Configure Parameters

Click the parameter configuration button to adjust evaluation parameters:
- Set sampling ratio and sample number limit.
- Adjust inference parameters (recommend using low temperature).
- Confirm evaluation sample file path.

### 3. Execute Evaluation

1. Click "Start Evaluation" button.
2. System loads checkpoint and initializes model.
3. Perform inference sample by sample.
4. Calculate similarity between model output and standard answer.
5. Generate evaluation report.

### 4. View Results

After evaluation is complete, display:
- **Overall Score**: Average similarity score of all samples.
- **Detailed Results**: Question, standard answer, model output, and score for each sample.
- **Comparison Chart**: Score comparison when evaluating multiple checkpoints.

---

## Evaluation Metrics

### Similarity Calculation

Evaluation uses text similarity algorithms to calculate the match degree between model output and standard answer:

| Factor | Weight |
|------|------|
| Semantic Similarity | Main metric |
| Keyword Coverage | Auxiliary metric |
| Format Match | Reference metric |

### Score Interpretation

| Score Range | Description |
|----------|------|
| 0.8 - 1.0 | Excellent, model output is highly consistent with standard answer |
| 0.6 - 0.8 | Good, model basically understands task requirements |
| 0.4 - 0.6 | Average, model partially understands task |
| 0.0 - 0.4 | Poor, model output deviates from expectation |

---

## Usage Suggestions

### Evaluation Strategy

1. **Regular Evaluation**
   - Evaluate every certain number of training steps (e.g., 100-500 steps).
   - Track changes in model quality with training progress.

2. **Comparative Evaluation**
   - Select multiple checkpoints to evaluate simultaneously.
   - Find the best training node.

3. **Comprehensive Evaluation**
   - Use all samples (sampling ratio 1.0) when finally selecting a checkpoint.
   - Obtain more reliable evaluation results.

### Sample Design

1. **Representativeness**
   - Samples should cover main scenarios of training data.
   - Include different types of questions.

2. **Standardization**
   - Unified answer format.
   - Facilitate objective scoring.

3. **Update and Maintenance**
   - Adjust samples according to actual needs.
   - Regularly clean up obsolete samples.

---

## Difference from Test Module

| Dimension | Evaluation Module | Test Module |
|------|----------|----------|
| Purpose | Quantitatively evaluate checkpoint quality | Interactively verify model effect |
| Method | Automated batch testing | Manually input questions for testing |
| Output | Score and evaluation report | Model's real-time reply |
| Usage Scenario | Screen best checkpoints | Confirm model actual performance |

**Recommended Workflow**:
1. First use Evaluation Module to find checkpoints with higher scores.
2. Then use Test Module to actually verify the answer quality of these checkpoints.
3. Finally determine the checkpoint for packaging.

---

## Next Steps

- [Overview](./overview.md) - Training function overview
- [Prepare Module Details](./prepare.md) - Task creation and parameter configuration
- [Monitor Module Details](./monitor.md) - Training monitoring
- [Test Module Details](./test.md) - Interactive testing
- [Package Module Details](./package.md) - GGUF export
