# Training Test Module Details

> v1.0.0
> 2025-12

> ðŸ“Œ **Module Positioning**: Interactively test the actual inference effect of checkpoints and verify the answer quality of the model in real dialogue scenarios.

## Table of Contents
- [Overview](#overview)
- [Usage Scenarios](#usage-scenarios)
- [Inference Parameters](#inference-parameters)
  - [Basic Parameters](#basic-parameters)
  - [Advanced Parameters](#advanced-parameters)
- [Test Flow](#test-flow)
- [Test Suggestions](#test-suggestions)
- [FAQ](#faq)
- [Relationship with Other Modules](#relationship-with-other-modules)
- [Next Steps](#next-steps)

---

## Overview

The Test module provides an interactive dialogue interface, allowing users to manually input questions and view the model's actual replies to intuitively verify training effects.

### Core Functions

| Function | Description |
|------|------|
| Checkpoint Selection | Select training checkpoint to test |
| Interactive Dialogue | Input questions to get model replies |
| Parameter Adjustment | Adjust inference parameters to observe effect changes |
| Multi-turn Dialogue | Support continuous multi-turn dialogue testing |

---

## Usage Scenarios

### Typical Workflow

1. **Verification after Evaluation**: After the Evaluation module screens out high-scoring checkpoints, use the Test module to actually experience them.
2. **Comparative Testing**: Switch different checkpoints to compare answer quality for the same question.
3. **Boundary Testing**: Test model performance in extreme or edge cases.
4. **Parameter Tuning**: Adjust inference parameters to find the best generation effect.

### Cooperation with Evaluation Module

```
Evaluation Module â†’ Find High Score Checkpoints â†’ Test Module Verification â†’ Determine Final Checkpoint
```

---

## Inference Parameters

### Basic Parameters

#### Temperature

| Attribute | Value |
|------|-----|
| Type | Float |
| Default | 0.7 |
| Range | 0 - 2 |

**Description**: Control randomness and creativity of output.

| Temperature Value | Effect |
|--------|------|
| 0 - 0.3 | Conservative, high certainty, suitable for factual Q&A |
| 0.4 - 0.7 | Balanced, suitable for daily conversation |
| 0.8 - 1.2 | High creativity, suitable for creative writing |
| > 1.2 | Highly random, may be incoherent |

---

#### Max length

| Attribute | Value |
|------|-----|
| Type | Integer |
| Default | 256 |
| Range | 1 - 8192 |

**Description**: Maximum number of tokens generated by the model.

**Setting Suggestions**:
- Short Answer: 128 - 256
- Detailed Explanation: 512 - 1024
- Long Text Generation: 2048+

---

#### Top P (Nucleus Sampling)

| Attribute | Value |
|------|-----|
| Type | Float |
| Default | 0.9 |
| Range | 0 - 1 |

**Description**: Sample from candidate words with cumulative probability reaching P.

| Top P Value | Effect |
|----------|------|
| 0.1 - 0.5 | Conservative, only select high probability words |
| 0.7 - 0.9 | Balanced, recommended value |
| 0.95 - 1.0 | Loose, more diverse |

---

#### Top K

| Attribute | Value |
|------|-----|
| Type | Integer |
| Default | 50 |
| Range | 0 - 100 |

**Description**: Sample only from the K highest probability words.

**Usage Suggestions**:
- Set to 0 means do not use Top K limit.
- Usually 20-50 is a good choice.
- Works better when used with Top P.

---

#### Repetition penalty

| Attribute | Value |
|------|-----|
| Type | Float |
| Default | 1.1 |
| Range | 0 - 2 |

**Description**: Suppress model from repeating generated content.

| Value | Effect |
|----|------|
| 1.0 | No penalty |
| 1.1 - 1.2 | Slight suppression, recommended |
| 1.3 - 1.5 | Obvious suppression |
| > 1.5 | Strong suppression, may affect fluency |

---

#### Presence penalty

| Attribute | Value |
|------|-----|
| Type | Float |
| Default | 1.1 |
| Range | 0 - 2 |

**Description**: Penalize tokens that have already appeared, encouraging model to use new vocabulary.

**Difference from Repetition Penalty**:
- Repetition Penalty: Penalize continuous repetition.
- Presence Penalty: Penalize any word that has appeared.

---

#### Min P

| Attribute | Value |
|------|-----|
| Type | Float |
| Default | 0.0 |
| Range | 0 - 1 |

**Description**: Filter out words with probability lower than threshold.

**Usage Scenarios**:
- Set to 0 means do not use.
- Set to 0.05-0.1 can filter low probability noise.

---

#### Random seed

| Attribute | Value |
|------|-----|
| Type | Integer |
| Default | 42 |
| Range | 0 - 999999 |

**Description**: Fixed random seed can reproduce the same output.

**Usage Scenarios**:
- Use same seed during comparative testing to ensure fairness.
- Fix seed during debugging to facilitate problem reproduction.

---

### Advanced Parameters

#### Enable thinking

| Attribute | Value |
|------|-----|
| Type | Boolean |
| Default | false |

**Description**: Enable model's "thinking" capability (if model supports).

**Applicable Models**: Models trained with thinking tags, such as QwQ series.

---

#### System prompt

| Attribute | Value |
|------|-----|
| Type | Text |
| Default | Empty |
| Max Length | 1000 characters |

**Description**: Set model role and behavior guidance.

**Example**:
```
You are a professional customer service assistant, please answer user questions in a friendly and professional tone.
```

---

## Test Flow

### 1. Select Checkpoint

Select the checkpoint to test from the task's checkpoint list:
- Can select based on recommendation from Evaluation module.
- Can select checkpoints with different training steps for comparison.

### 2. Configure Parameters

Click parameter configuration button:
- Adjust inference parameters.
- Set system prompt (optional).
- Configure thinking mode (optional).

### 3. Start Dialogue

In the dialogue box:
1. Input test question.
2. Wait for model to generate reply.
3. View reply content.
4. Continue to input next question (multi-turn dialogue).

### 4. Analyze Results

Focus on the following aspects:
- **Answer Accuracy**: Whether the question is answered correctly.
- **Answer Fluency**: Whether the language is natural and smooth.
- **Format Standardization**: Whether it meets expected output format.
- **Knowledge Correctness**: Whether it contains correct knowledge.

---

## Test Suggestions

### Test Case Design

| Type | Description | Example |
|------|------|------|
| Normal Scenario | Typical questions covered by training data | Standard Q&A pairs |
| Boundary Scenario | Questions on the edge of training data | Rare vocabulary, complex syntax |
| Abnormal Scenario | Questions outside training data | Test model's refusal capability |
| Comparative Scenario | Compare with base model | Verify fine-tuning effect |

### Parameter Tuning Strategy

1. **Start from Default**
   - Use default parameters to get baseline effect.

2. **Adjust One by One**
   - Adjust only one parameter at a time.
   - Observe effect changes.

3. **Record Best Combination**
   - Find the parameter combination with best effect.
   - Used for subsequent packaging configuration.

### Multi-Checkpoint Comparison

1. Prepare a set of standard test questions.
2. Use same parameters and random seed.
3. Test different checkpoints sequentially.
4. Record and compare answer quality.

---

## FAQ

### Incomplete Answer

**Possible Reasons**:
- Max length setting too small.
- Model generated EOS token.

**Solution**:
- Increase max length parameter.
- Check end token format in training data.

### Repetitive Answer

**Possible Reasons**:
- Repetition penalty too low.
- Temperature too high causing unstable sampling.

**Solution**:
- Increase repetition penalty (e.g., 1.2-1.3).
- Appropriately lower temperature.

### Answer Off-Topic

**Possible Reasons**:
- Training data quality issue.
- Improper inference parameters.

**Solution**:
- Lower temperature to get more deterministic answer.
- Check relevance of training data.

### Checkpoint Load Failed

**Possible Reasons**:
- Checkpoint file incomplete.
- Insufficient memory.

**Solution**:
- Confirm checkpoint directory contains necessary files.
- Close other programs occupying memory.

---

## Relationship with Other Modules

```
Prepare Module â†’ Monitor Module â†’ Evaluation Module â†’ Test Module â†’ Package Module
   â”‚              â”‚               â”‚               â”‚             â”‚
 Create Task    Monitor Progress  Quantify Eval   Actual Verify  Export Model
```

**Positioning of Test Module**:
- After Evaluation Module: Select checkpoint based on evaluation score.
- Before Package Module: Confirm checkpoint quality before packaging.

---

## Next Steps

- [Overview](./overview.md) - Training function overview
- [Prepare Module Details](./prepare.md) - Task creation and parameter configuration
- [Monitor Module Details](./monitor.md) - Training monitoring
- [Evaluate Module Details](./evaluate.md) - Checkpoint evaluation
- [Package Module Details](./package.md) - GGUF export
